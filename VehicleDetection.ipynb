{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Project 5 : Vehicle Detection and Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import csv\n",
    "import cv2\n",
    "import glob\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import random as rand\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from skimage.feature import hog\n",
    "from sklearn.model_selection import train_test_split\n",
    "from skimage.feature import hog\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for extracting Features from images \n",
    "Below is the function get_hog_features from lesson 34[lesson_functions.py] of Project:Vehicle Detection and Tracking  which will return hog features and visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_hog_features(img, orient, pix_per_cell, cell_per_block, \n",
    "                        vis=False, feature_vec=True):\n",
    "    # Call with two outputs if vis==True\n",
    "    if vis == True:\n",
    "        features, hog_image = hog(img, orientations=orient, \n",
    "                                  pixels_per_cell=(pix_per_cell, pix_per_cell),\n",
    "                                  cells_per_block=(cell_per_block, cell_per_block), \n",
    "                                  transform_sqrt=True, \n",
    "                                  visualise=vis, feature_vector=feature_vec)\n",
    "        return features, hog_image\n",
    "    # Otherwise call with one output\n",
    "    else:      \n",
    "        features = hog(img, orientations=orient, \n",
    "                       pixels_per_cell=(pix_per_cell, pix_per_cell),\n",
    "                       cells_per_block=(cell_per_block, cell_per_block), \n",
    "                       transform_sqrt=True, \n",
    "                       visualise=vis, feature_vector=feature_vec)\n",
    "        return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Below is the function bin_spatial from lesson 22 of Project: Vehicle Detection and Tracking which will return the computed binned color feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bin_spatial(img, size=(32, 32)):\n",
    "    # Use cv2.resize().ravel() to create the feature vector\n",
    "    features = cv2.resize(img, size).ravel() \n",
    "    # Return the feature vector\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the function color_hist from lesson 22 of Project: Vehicle Detection and Tracking which will return the computed color histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def color_hist(img, nbins=32, bins_range=(0, 1)):\n",
    "    # Compute the histogram of the color channels separately\n",
    "    channel1_hist = np.histogram(img[:,:,0], bins=nbins, range=bins_range)\n",
    "    channel2_hist = np.histogram(img[:,:,1], bins=nbins, range=bins_range)\n",
    "    channel3_hist = np.histogram(img[:,:,2], bins=nbins, range=bins_range)\n",
    "    # Concatenate the histograms into a single feature vector\n",
    "    hist_features = np.concatenate((channel1_hist[0], channel2_hist[0], channel3_hist[0]))\n",
    "    # Return the individual histograms, bin_centers and feature vector\n",
    "    return hist_features\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the function single_img_features taken from chapter 34 which will  will  extract spatial, color and hog features from single image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def single_img_features(img, color_space='RGB', spatial_size=(32, 32),\n",
    "                        hist_bins=32, orient=9, \n",
    "                        pix_per_cell=8, cell_per_block=2, hog_channel=0,\n",
    "                        spatial_feat=True, hist_feat=True, hog_feat=True):    \n",
    "    \"\"\"Extract spatial, color and hog features from single image\n",
    "    Args:\n",
    "        img (numpy.array): image in RGB format\n",
    "        color_space: GRAY, RGB, HSV, LUV, HLS, YUV, YCrCb\n",
    "        spatial_size (tuple): resize img before calculating spatial features\n",
    "            default value is (32, 32)\n",
    "        hist_bins (int): number of histogram bins, 32 by default\n",
    "        orient (int): number of HOG orientations\n",
    "        pix_per_cell (int): number of pixels in HOG cell\n",
    "        cell_per_block (int): number of HOG cells in block\n",
    "        hog_channel (int): channel to use for HOG features calculating, default 0\n",
    "        spatial_feat (boolean): calculate spatial featues, default True\n",
    "        hist_feat (boolean): calculate histogram featues, default True\n",
    "        hog_feat (boolean): calculate HOG featues, default True\n",
    "    Returns:\n",
    "        features_vector (list(numpy.array)): list of feature vectors\n",
    "    \"\"\"\n",
    "    #1) Define an empty list to receive features\n",
    "    img_features = []\n",
    "    #2) Apply color conversion if other than 'RGB'\n",
    "    if color_space != 'RGB':\n",
    "        feature_image = cv2.cvtColor (img, getattr(cv2, 'COLOR_RGB2' + color_space))\n",
    "    else: feature_image = np.copy(img)      \n",
    "    #3) Compute spatial features if flag is set\n",
    "    if spatial_feat == True:\n",
    "        spatial_features = bin_spatial(feature_image, size=spatial_size)\n",
    "        #4) Append features to list\n",
    "        img_features.append(spatial_features)\n",
    "    #5) Compute histogram features if flag is set\n",
    "    if hist_feat == True:\n",
    "        hist_features = color_hist(feature_image, nbins=hist_bins)\n",
    "        #6) Append features to list\n",
    "        img_features.append(hist_features)\n",
    "    #7) Compute HOG features if flag is set\n",
    "    if hog_feat == True:\n",
    "        if color_space == 'GRAY':\n",
    "            hog_features = get_hog_features(feature_image, orient, \n",
    "                        pix_per_cell, cell_per_block, vis=False, feature_vec=True)\n",
    "        elif hog_channel == 'ALL':\n",
    "            hog_features = []\n",
    "            for channel in range(feature_image.shape[2]):\n",
    "                hog_features.extend(get_hog_features(feature_image[:,:,channel], \n",
    "                                    orient, pix_per_cell, cell_per_block, \n",
    "                                    vis=False, feature_vec=True))      \n",
    "        else:\n",
    "            hog_features = get_hog_features(feature_image[:,:,hog_channel], orient, \n",
    "                        pix_per_cell, cell_per_block, vis=False, feature_vec=True)\n",
    "        #8) Append features to list\n",
    "        img_features.append(hog_features)\n",
    "\n",
    "    #9) Return concatenated array of features\n",
    "    return np.concatenate(img_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the function extract_features(taken from chapter 34[lesson_functions.py]) which will  extract spatial, color and hog features from list of images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def extract_features(imgs, color_space='RGB', spatial_size=(32, 32),\n",
    "                        hist_bins=32, orient=9, \n",
    "                        pix_per_cell=8, cell_per_block=2, hog_channel=0,\n",
    "                        spatial_feat=True, hist_feat=True, hog_feat=True):\n",
    "\n",
    "    # Create a list to append feature vectors to\n",
    "    features = []\n",
    "    # Iterate through the list of images\n",
    "    for image in imgs:\n",
    "        file_features = []\n",
    "        # apply color conversion if other than 'RGB'\n",
    "        if color_space != 'RGB':\n",
    "            if color_space == 'HSV':\n",
    "                feature_image = cv2.cvtColor(image, cv2.COLOR_RGB2HSV)\n",
    "            elif color_space == 'LUV':\n",
    "                feature_image = cv2.cvtColor(image, cv2.COLOR_RGB2LUV)\n",
    "            elif color_space == 'HLS':\n",
    "                feature_image = cv2.cvtColor(image, cv2.COLOR_RGB2HLS)\n",
    "            elif color_space == 'YUV':\n",
    "                feature_image = cv2.cvtColor(image, cv2.COLOR_RGB2YUV)\n",
    "            elif color_space == 'YCrCb':\n",
    "                feature_image = cv2.cvtColor(image, cv2.COLOR_RGB2YCrCb)\n",
    "            elif color_space ==\"GRAY\":\n",
    "                feature_image = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "        else: \n",
    "            feature_image = np.copy(image) \n",
    "        if spatial_feat == True:\n",
    "            spatial_features = bin_spatial(feature_image, size=spatial_size)\n",
    "            file_features.append(spatial_features)\n",
    "        if hist_feat == True:\n",
    "            # Apply color_hist()\n",
    "            hist_features = color_hist(feature_image, nbins=hist_bins)\n",
    "            file_features.append(hist_features)\n",
    "        if hog_feat == True:\n",
    "        # Call get_hog_features() with vis=False, feature_vec=True\n",
    "            if color_space == 'GRAY':\n",
    "                hog_features = get_hog_features(feature_image, orient, \n",
    "                            pix_per_cell, cell_per_block, vis=False, feature_vec=True)\n",
    "            elif hog_channel == 'ALL':\n",
    "                hog_features = []\n",
    "                for channel in range(feature_image.shape[2]):\n",
    "                    hog_features.append(get_hog_features(feature_image[:,:,channel], \n",
    "                                        orient, pix_per_cell, cell_per_block, \n",
    "                                        vis=False, feature_vec=True))\n",
    "                hog_features = np.ravel(hog_features)        \n",
    "            else:\n",
    "                hog_features = get_hog_features(feature_image[:,:,hog_channel], orient, \n",
    "                            pix_per_cell, cell_per_block, vis=False, feature_vec=True)\n",
    "            # Append the new feature vector to the features list\n",
    "            file_features.append(hog_features)\n",
    "        features.append(np.concatenate(file_features))\n",
    "    # Return list of feature vectors\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Loading Vehicle and Non-Vehicle Images\n",
    "The following code will read images from both vehicle and non-vehicle folder in resources folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8792 images of car found\n",
      "8968 images are non-car images\n"
     ]
    }
   ],
   "source": [
    "#Loading vehicle and non-vehicle images\n",
    "\n",
    "cars = glob.glob('resources/vehicles/**/*.png', recursive=True)\n",
    "notcars = glob.glob('resources/non-vehicles/**/*.png', recursive=True)\n",
    "\n",
    "images_of_car = [mpimg.imread(impath) for impath in cars]\n",
    "images_of_noncar = [mpimg.imread(impath) for impath in notcars]\n",
    "\n",
    "print(\"{0} images of car found\".format(len(images_of_car)))\n",
    "print(\"{0} images are non-car images\".format(len(images_of_noncar)))\n",
    "\n",
    "      \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Udacity's  Annotated Driving Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#This if for CrowdAI dataset\n",
    "# crowdai_car_count=15000\n",
    "# crowdai_noncar_count = 25000\n",
    "# crowdai_image_dict={}\n",
    "# number_of_cars = 0\n",
    "# with open('resources/crowdai/labels1.csv') as csvfile:\n",
    "#     reader = csv.reader(csvfile, delimiter=',', quotechar='|')\n",
    "#     for row in reader:\n",
    "#         frame = row [4]\n",
    "#         if frame not in crowdai_image_dict:\n",
    "#             image = {'impath' : 'resources/crowdai/' + frame,'car_boxes' : []}\n",
    "#             crowdai_image_dict [frame] = image\n",
    "#         else:\n",
    "#             image = crowdai_image_dict [frame]\n",
    "            \n",
    "#         if (row[5] == 'Car'):\n",
    "#             car_data = {\n",
    "#                 'x1' : int(row [1]),\n",
    "#                 'y1' : int(row [3]),\n",
    "#                 'x2' : int(row [0]),\n",
    "#                 'y2' : int(row [2])\n",
    "#             }\n",
    "#             image['car_boxes'].append (car_data)\n",
    "#             number_of_cars += 1\n",
    "\n",
    "# print(\"Number of images in crowdai dataset = {0}\".format(len(crowdai_image_dict)))\n",
    "# print(\"Number of cars detected = {0}\".format(number_of_cars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images in autti dataset = 13063\n",
      "Number of cars detected = 60788\n"
     ]
    }
   ],
   "source": [
    "autti_car_count=15000\n",
    "autti_noncar_count = 25000\n",
    "autti_image_dict={}\n",
    "number_of_cars = 0\n",
    "\n",
    "#Make Sure to keep the autti dataset in resources folder\n",
    "with open('resources/autti/labels.csv', newline ='') as labelcsv:\n",
    "    reader = csv.reader(labelcsv, delimiter=' ', quotechar='|')\n",
    "    for row in reader:\n",
    "        frame = row [0]\n",
    "        if frame not in autti_image_dict:\n",
    "            image = {'impath' : 'resources/autti/' + frame,'car_boxes' : []}\n",
    "            autti_image_dict [frame] = image\n",
    "        else:\n",
    "            image = autti_image_dict [frame]\n",
    "            \n",
    "        if (row[6] == '\"car\"'):\n",
    "            car_data = {\n",
    "                'x1' : int(row [1]),\n",
    "                'y1' : int(row [2]),\n",
    "                'x2' : int(row [3]),\n",
    "                'y2' : int(row [4])\n",
    "            }\n",
    "            image['car_boxes'].append (car_data)\n",
    "            number_of_cars += 1\n",
    "\n",
    "print(\"Number of images in autti dataset = {0}\".format(len(autti_image_dict)))\n",
    "print(\"Number of cars detected = {0}\".format(number_of_cars))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Croping Images in crowdAI dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 500/500 [01:01<00:00,  7.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " cars: 492\n",
      "non cars: 492\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def non_vehicle_box (image_size, car_boxes):\n",
    "    non_car_box_size = [128, 64, 48]\n",
    "    while True:\n",
    "        box_size = non_car_box_size [rand.randint (0, len (non_car_box_size)-1)]\n",
    "        box_size = [box_size, box_size]\n",
    "        temp = [image_size[0] - box_size[0],image_size[1] - box_size[1],]\n",
    "        position = [rand.randint (0, temp[0] - 1),rand.randint (0, temp[1] - 1)]\n",
    "        x1, x2, y1, y2 = position [0],position [0] + box_size [0],  position [1], position [1] + box_size [1]\n",
    "        for each in car_boxes:\n",
    "            #Checking for intersection \n",
    "            boxx1, boxx2, boxy1, boxy2 = each['x1'],each['x2'], each['y1'], each['y2']\n",
    "            if ((((x1 >= boxx1 and x1 <= boxx2) or (x2 >= boxx1 and x2 <= boxx2)) and\n",
    "                ((y1 >= boxy1 and y1 <= boxy2) or (y2 >= boxy1 and y2 <= boxy2))) or\n",
    "            (((boxx1 >= x1 and boxx1 <= x2) or (boxx2 >= x1 and boxx2 <= x2)) and\n",
    "                ((boxy1 >= y1 and boxy1 <= y2) or (boxy2 >= y1 and boxy2 <= y2)))):\n",
    "                pass\n",
    "            else:\n",
    "                return {'x1':x1,'y1': y1,'x2': x2,'y2': y2}\n",
    "\n",
    "#For crowdai dataset\n",
    "# def crop_crowdai_dataset (no_of_cars, no_of_non_cars):\n",
    "#     images_last_index = len (crowdai_image_dict) - 1\n",
    "#     cars_crowdai = []\n",
    "#     non_cars_crowdai = []\n",
    "#     frames = list(crowdai_image_dict.keys ())\n",
    "    \n",
    "#     for i in tqdm(range (max (no_of_cars, no_of_non_cars))):\n",
    "#         image = crowdai_image_dict [frames[rand.randint (0, len(crowdai_image_dict)-1)]]\n",
    "#         if len(image ['car_boxes']) == 0:\n",
    "#             continue\n",
    "        \n",
    "        \n",
    "\n",
    "#         im = mpimg.imread(image ['impath'])\n",
    "#         im = im.astype(np.float32)/255\n",
    "#         '''Had to keep the try catch block because opencv has a bug\n",
    "#             see the below link for the fix\n",
    "#            https://github.com/opencv/opencv/pull/6883/commits/5bc10ef796c29563e8916ff31b5c1a262422a93f\n",
    "#         '''\n",
    "#         try:\n",
    "        \n",
    "#             if(i < no_of_cars):\n",
    "#                 car_box = image['car_boxes'][rand.randint(0, len(image ['car_boxes'])-1)]\n",
    "#                 car_image = im[car_box['y1']:car_box['y2'], car_box['x1']:car_box['x2']]\n",
    "#                 car_image = cv2.resize(car_image, (64, 64), interpolation=cv2.INTER_AREA)\n",
    "#                 cars_crowdai.append (car_image)\n",
    "\n",
    "\n",
    "#             if(i < no_of_non_cars):\n",
    "#                 non_car_box = non_vehicle_box ([im.shape[1], im.shape[0]], image['car_boxes'])\n",
    "#                 non_car_image = im[non_car_box['y1']:non_car_box['y2'], non_car_box['x1']:non_car_box['x2']]\n",
    "#                 non_car_image = cv2.resize(non_car_image, (64, 64), interpolation=cv2.INTER_AREA)\n",
    "#                 non_cars_crowdai.append (non_car_image)\n",
    "#         except:\n",
    "#             pass\n",
    "        \n",
    "#     return(cars_crowdai, non_cars_crowdai)\n",
    "# cars_crowdai, non_cars_crowdai = crop_crowdai_dataset (crowdai_car_count, crowdai_noncar_count)\n",
    "# print (' cars:', len (cars_crowdai))\n",
    "# print ('non cars:', len (non_cars_crowdai))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def crop_autti_dataset (no_of_cars, no_of_non_cars):\n",
    "    images_last_index = len (autti_image_dict) - 1\n",
    "    cars_autti = []\n",
    "    non_cars_autti = []\n",
    "    frames = list(autti_image_dict.keys ())\n",
    "    \n",
    "    for i in tqdm(range (max (no_of_cars, no_of_non_cars))):\n",
    "        image = autti_image_dict [frames[rand.randint (0, len(autti_image_dict)-1)]]\n",
    "        if len(image ['car_boxes']) == 0:\n",
    "            continue\n",
    "        im = mpimg.imread(image ['impath'])\n",
    "        im = im.astype(np.float32)/255\n",
    "        '''Had to keep the try catch block because opencv has a bug\n",
    "            see the below link for the fix\n",
    "           https://github.com/opencv/opencv/pull/6883/commits/5bc10ef796c29563e8916ff31b5c1a262422a93f\n",
    "        '''\n",
    "        if(i < no_of_cars):\n",
    "            car_box = image['car_boxes'][rand.randint(0, len(image ['car_boxes'])-1)]\n",
    "            car_image = im[car_box['y1']:car_box['y2'], car_box['x1']:car_box['x2']]\n",
    "            car_image = cv2.resize(car_image, (64, 64), interpolation=cv2.INTER_AREA)\n",
    "            cars_autti.append (car_image)\n",
    "\n",
    "\n",
    "        if(i < no_of_non_cars):\n",
    "            non_car_box = non_vehicle_box ([im.shape[1], im.shape[0]], image['car_boxes'])\n",
    "            non_car_image = im[non_car_box['y1']:non_car_box['y2'], non_car_box['x1']:non_car_box['x2']]\n",
    "            non_car_image = cv2.resize(non_car_image, (64, 64), interpolation=cv2.INTER_AREA)\n",
    "            non_cars_autti.append (non_car_image)\n",
    "\n",
    "    return(cars_autti, non_cars_autti)\n",
    "\n",
    "cars_autti, non_cars_autti = crop_autti_dataset (autti_car_count, autti_noncar_count)\n",
    "print (' cars:', len (cars_autti))\n",
    "print ('non cars:', len (non_cars_autti))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting Features, Normalizing Features, Training the Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Anaconda3\\envs\\py35\\lib\\site-packages\\skimage\\feature\\_hog.py:119: skimage_deprecation: Default value of `block_norm`==`L1` is deprecated and will be changed to `L2-Hys` in v0.15\n",
      "  'be changed to `L2-Hys` in v0.15', skimage_deprecation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features extraction time:  21.25\n",
      "Using: 8 orientations 16 pixels per cell and 1 cells per block\n",
      "Feature vector length: 128\n",
      "Test Accuracy of SVC =  0.9829\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# parameters of feature extraction\n",
    "\n",
    "color_space = 'GRAY' # Can be GRAY, RGB, HSV, LUV, HLS, YUV, YCrCb\n",
    "orient = 8  # HOG orientations\n",
    "pix_per_cell = 16 # HOG pixels per cell\n",
    "cell_per_block = 1 # HOG cells per block\n",
    "hog_channel = 'ALL' # Can be 0, 1, 2, or \"ALL\"\n",
    "spatial_size = (16, 16) # Spatial binning dimensions\n",
    "hist_bins = 16    # Number of histogram bins\n",
    "spatial_feat = False # Spatial features on or off\n",
    "hist_feat = False # Histogram features on or off\n",
    "hog_feat = True # HOG features on or off\n",
    "\n",
    "#For crowdai dataset\n",
    "# # Combining crowdai, vehicle and non-vehicle resources\n",
    "# all_cars = []\n",
    "# all_cars.extend(cars)\n",
    "# all_cars.extend(cars_crowdai)\n",
    "\n",
    "# all_non_cars = []\n",
    "# all_non_cars.extend(notcars)\n",
    "# all_non_cars.extend(non_cars_crowdai)\n",
    "\n",
    "\n",
    "#for autti dataset\n",
    "\n",
    "all_cars = []\n",
    "all_cars.extend(images_of_car)\n",
    "all_cars.extend(cars_autti)\n",
    "\n",
    "all_non_cars = []\n",
    "all_non_cars.extend(images_of_noncar)\n",
    "all_non_cars.extend(non_cars_autti)\n",
    "\n",
    "\n",
    "\n",
    "#Extracting Features\n",
    "ft=time.time()\n",
    "car_features = extract_features(all_cars, color_space=color_space, \n",
    "                        spatial_size=spatial_size, hist_bins=hist_bins, \n",
    "                        orient=orient, pix_per_cell=pix_per_cell, \n",
    "                        cell_per_block=cell_per_block, \n",
    "                        hog_channel=hog_channel, spatial_feat=spatial_feat, \n",
    "                        hist_feat=hist_feat, hog_feat=hog_feat)\n",
    "notcar_features = extract_features(all_non_cars, color_space=color_space, \n",
    "                        spatial_size=spatial_size, hist_bins=hist_bins, \n",
    "                        orient=orient, pix_per_cell=pix_per_cell, \n",
    "                        cell_per_block=cell_per_block, \n",
    "                        hog_channel=hog_channel, spatial_feat=spatial_feat, \n",
    "                        hist_feat=hist_feat, hog_feat=hog_feat)\n",
    "ft2=time.time()\n",
    "print ('features extraction time: ', round(ft2-ft, 2))\n",
    "\n",
    "\n",
    "#Normalizing features\n",
    "X = np.vstack((car_features, notcar_features)).astype(np.float64)                        \n",
    "\n",
    "X_scaler = StandardScaler().fit(X)\n",
    "\n",
    "scaled_X = X_scaler.transform(X)\n",
    "\n",
    "\n",
    "y = np.hstack((np.ones(len(car_features)), np.zeros(len(notcar_features))))\n",
    "\n",
    "\n",
    "# Split up data into randomized training and test sets\n",
    "rand_state = np.random.randint(0, 100)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    scaled_X, y, test_size=0.2, random_state=rand_state)\n",
    "\n",
    "print('Using:',orient,'orientations',pix_per_cell,\n",
    "    'pixels per cell and', cell_per_block,'cells per block')\n",
    "print('Feature vector length:', len(X_train[0]))\n",
    "\n",
    "#Training the classifier\n",
    "#Test Accuracy of LinearSVC =  0.9384  \n",
    "#Test Accuracy of LinearSVC(loss = 'hinge') =  0.9419\n",
    "#SVC is showing better Accuracy, so taking SVC\n",
    "svc = SVC()\n",
    "\n",
    "# Timing the SVC\n",
    "t=time.time()\n",
    "svc.fit(X_train, y_train)\n",
    "t2 = time.time()\n",
    "# Check the score of the SVC\n",
    "print('Test Accuracy of SVC = ', round(svc.score(X_test, y_test), 4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Sliding Window\n",
    "\n",
    "The below function slide_window is taken from Chapter 32 of Vehicle Detection and Tracking Lesson.\n",
    "This function takes an image,start and stop positions in both x and y,window size (x and y dimensions) and overlap fraction (for both x and y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def slide_window(img, x_start_stop=[None, None], y_start_stop=[None, None], \n",
    "                    xy_window=(64, 64), xy_overlap=(0.5, 0.5)):\n",
    "    # If x and/or y start/stop positions not defined, set to image size\n",
    "    if x_start_stop[0] == None:\n",
    "        x_start_stop[0] = 0\n",
    "    if x_start_stop[1] == None:\n",
    "        x_start_stop[1] = img.shape[1]\n",
    "    if y_start_stop[0] == None:\n",
    "        y_start_stop[0] = 0\n",
    "    if y_start_stop[1] == None:\n",
    "        y_start_stop[1] = img.shape[0]\n",
    "    # Compute the span of the region to be searched    \n",
    "    xspan = x_start_stop[1] - x_start_stop[0]\n",
    "    yspan = y_start_stop[1] - y_start_stop[0]\n",
    "    # Compute the number of pixels per step in x/y\n",
    "    nx_pix_per_step = np.int(xy_window[0]*(1 - xy_overlap[0]))\n",
    "    ny_pix_per_step = np.int(xy_window[1]*(1 - xy_overlap[1]))\n",
    "    # Compute the number of windows in x/y\n",
    "    nx_buffer = np.int(xy_window[0]*(xy_overlap[0]))\n",
    "    ny_buffer = np.int(xy_window[1]*(xy_overlap[1]))\n",
    "    nx_windows = np.int((xspan-nx_buffer)/nx_pix_per_step) \n",
    "    ny_windows = np.int((yspan-ny_buffer)/ny_pix_per_step) \n",
    "    # Initialize a list to append window positions to\n",
    "    window_list = []\n",
    "    # Loop through finding x and y window positions\n",
    "    # Note: you could vectorize this step, but in practice\n",
    "    # you'll be considering windows one by one with your\n",
    "    # classifier, so looping makes sense\n",
    "    for ys in range(ny_windows):\n",
    "        for xs in range(nx_windows):\n",
    "            # Calculate window position\n",
    "            startx = xs*nx_pix_per_step + x_start_stop[0]\n",
    "            endx = startx + xy_window[0]\n",
    "            starty = ys*ny_pix_per_step + y_start_stop[0]\n",
    "            endy = starty + xy_window[1]\n",
    "            # Append window position to list\n",
    "            window_list.append(((startx, starty), (endx, endy)))\n",
    "    # Return the list of windows\n",
    "    return window_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def search_windows(img, windows, clf, scaler, color_space='RGB', \n",
    "                    spatial_size=(32, 32), hist_bins=32, \n",
    "                    hist_range=(0, 256), orient=9, \n",
    "                    pix_per_cell=8, cell_per_block=2, \n",
    "                    hog_channel=0, spatial_feat=True, \n",
    "                    hist_feat=True, hog_feat=True):\n",
    "    #1) Create an empty list to receive positive detection windows\n",
    "    on_windows = []\n",
    "    #2) Iterate over all windows in the list\n",
    "    for window in windows:\n",
    "        #3) Extract the test window from original image\n",
    "        test_img = cv2.resize(img[window[0][1]:window[1][1], window[0][0]:window[1][0]], (64, 64), interpolation=cv2.INTER_AREA)      \n",
    "        #4) Extract features for that window using single_img_features()\n",
    "        features = single_img_features(test_img, color_space=color_space, \n",
    "                            spatial_size=spatial_size, hist_bins=hist_bins, \n",
    "                            orient=orient, pix_per_cell=pix_per_cell, \n",
    "                            cell_per_block=cell_per_block, \n",
    "                            hog_channel=hog_channel, spatial_feat=spatial_feat, \n",
    "                            hist_feat=hist_feat, hog_feat=hog_feat)\n",
    "        #5) Scale extracted features to be fed to classifier\n",
    "\n",
    "        test_features = scaler.transform(np.array(features).reshape(1, -1))\n",
    "        #6) Predict using your classifier\n",
    "        prediction = clf.predict(test_features)\n",
    "        #7) If positive (prediction == 1) then save the window\n",
    "        if prediction == 1:\n",
    "            on_windows.append(window)\n",
    "    #8) Return windows for positive detections\n",
    "    return on_windows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to draw Boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def draw_boxes(img, bboxes, color=(0, 0, 255), thick=6):\n",
    "\n",
    "    # Make a copy of the image\n",
    "    imcopy = np.copy(img)\n",
    "    # Iterate through the bounding boxes\n",
    "    for bbox in bboxes:\n",
    "        # Draw a rectangle given bbox coordinates\n",
    "        cv2.rectangle(imcopy, bbox[0], bbox[1], color, thick)\n",
    "    # Return the image copy with boxes drawn\n",
    "    return imcopy\n",
    "\n",
    "def get_s_from_hls (img):\n",
    "    hls = cv2.cvtColor (img, cv2.COLOR_BGR2HLS)\n",
    "    return hls [:,:,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "image = mpimg.imread('test_images/test1.jpg')\n",
    "window_img = np.copy(image)\n",
    "\n",
    "sw_x_limits = [\n",
    "    [None, None],\n",
    "    [32, None],\n",
    "    [412, 1280]\n",
    "]\n",
    "\n",
    "sw_y_limits = [\n",
    "    [400, 640],\n",
    "    [400, 600],\n",
    "    [390, 540]\n",
    "]\n",
    "\n",
    "sw_window_size = [\n",
    "    (128, 128),\n",
    "    (96, 96),\n",
    "    (80, 80)\n",
    "]\n",
    "\n",
    "sw_overlap = [\n",
    "    (0.5, 0.5),\n",
    "    (0.5, 0.5),\n",
    "    (0.5, 0.5)\n",
    "]\n",
    "\n",
    "# create sliding windows\n",
    "windows = slide_window(image, x_start_stop=sw_x_limits[0], y_start_stop=sw_y_limits[0], \n",
    "                    xy_window=sw_window_size[0], xy_overlap=sw_overlap[0])\n",
    "\n",
    "windows2 = slide_window(image, x_start_stop=sw_x_limits[1], y_start_stop=sw_y_limits[1], \n",
    "                    xy_window=sw_window_size[1], xy_overlap=sw_overlap[1])\n",
    "\n",
    "windows3 = slide_window(image, x_start_stop=sw_x_limits[2], y_start_stop=sw_y_limits[2], \n",
    "                    xy_window=sw_window_size[2], xy_overlap=sw_overlap[2])\n",
    "\n",
    "# show sliding windows\n",
    "\n",
    "sliding_windows= draw_boxes(np.copy(image), windows, color=(0, 0, 0), thick=4)\n",
    "\n",
    "\n",
    "# drawing one of sliding windows in blue\n",
    "sliding_windows = draw_boxes (sliding_windows, [windows[9]], color=(0, 0, 255), thick=8)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_hot_boxes (image):\n",
    "\n",
    "\n",
    "    dst = np.copy (image)\n",
    "    all_hot_windows = []\n",
    "    \n",
    "    # iterate over previousely defined sliding windows\n",
    "    for x_limits, y_limits, window_size, overlap in zip (sw_x_limits, sw_y_limits, sw_window_size, sw_overlap):\n",
    "\n",
    "        windows = slide_window(\n",
    "            dst,\n",
    "            x_start_stop=x_limits,\n",
    "            y_start_stop=y_limits, \n",
    "            xy_window=window_size,\n",
    "            xy_overlap=overlap\n",
    "        )\n",
    "\n",
    "        hot_windows = search_windows(image, windows, svc, X_scaler, color_space=color_space, \n",
    "                            spatial_size=spatial_size, hist_bins=hist_bins, \n",
    "                            orient=orient, pix_per_cell=pix_per_cell, \n",
    "                            cell_per_block=cell_per_block, \n",
    "                            hog_channel=hog_channel, spatial_feat=spatial_feat, \n",
    "                            hist_feat=hist_feat, hog_feat=hog_feat)                       \n",
    "        \n",
    "        all_hot_windows.extend (hot_windows)\n",
    "\n",
    "        dst = draw_boxes(dst, hot_windows, color=(0, 0, 1), thick=4)\n",
    "\n",
    "    return all_hot_windows, dst\n",
    "        \n",
    "def get_heat_map(image, bbox_list):\n",
    "\n",
    "\n",
    "    heatmap = np.zeros_like(image[:,:,0]).astype(np.float)\n",
    "\n",
    "    # Iterate through list of bboxes\n",
    "    for box in bbox_list:\n",
    "        # Add += 1 for all pixels inside each bbox\n",
    "        heatmap[box[0][1]:box[1][1], box[0][0]:box[1][0]] += 1\n",
    "    \n",
    "    # Return updated heatmap\n",
    "    return heatmap\n",
    "\n",
    "class HotBox ():\n",
    "\n",
    "    def __init__ (self, box):\n",
    "        self.avg_box = [list(p) for p in box]\n",
    "        self.detected_count = 1\n",
    "        self.boxes = [box]\n",
    "    \n",
    "    def get_strength (self):\n",
    "\n",
    "        return self.detected_count\n",
    "    \n",
    "    def get_box (self):\n",
    "\n",
    "        if len(self.boxes) > 1:\n",
    "            center = np.average (np.average (self.boxes, axis=1), axis=0).astype(np.int32).tolist()\n",
    "\n",
    "            # getting all x and y coordinates of\n",
    "            # all corners of joined boxes separately\n",
    "            xs = np.array(self.boxes) [:,:,0]\n",
    "            ys = np.array(self.boxes) [:,:,1]\n",
    "\n",
    "            half_width = int(np.std (xs))\n",
    "            half_height = int(np.std (ys))\n",
    "            return (\n",
    "                (\n",
    "                    center[0] - half_width,\n",
    "                    center[1] - half_height\n",
    "                ), (\n",
    "                    center[0] + half_width,\n",
    "                    center[1] + half_height\n",
    "                ))\n",
    "        else:\n",
    "            return self.boxes [0]\n",
    "    \n",
    "    def is_close (self, box):\n",
    "        x11 = self.avg_box [0][0]\n",
    "        y11 = self.avg_box [0][1]\n",
    "        x12 = self.avg_box [1][0]\n",
    "        y12 = self.avg_box [1][1]\n",
    "        x21 = box [0][0]\n",
    "        y21 = box [0][1]\n",
    "        x22 = box [1][0]\n",
    "        y22 = box [1][1]\n",
    "            \n",
    "        x_overlap = max(0, min(x12,x22) - max(x11,x21))\n",
    "        y_overlap = max(0, min(y12,y22) - max(y11,y21))\n",
    "\n",
    "        area1 = (x12 - x11) * (y12 - y11)\n",
    "        area2 = (x22 - x21) * (y22 - y21)\n",
    "        intersection = x_overlap * y_overlap;\n",
    "        \n",
    "        if (\n",
    "            intersection >= 0.3 * area1 or\n",
    "            intersection >= 0.3 * area2\n",
    "        ):\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    \n",
    "    def join (self, boxes):\n",
    "        \n",
    "        joined = False\n",
    "        \n",
    "        for b in boxes:\n",
    "            if self.is_close (b):\n",
    "                boxes.remove (b)\n",
    "                self.boxes.append (b)\n",
    "                self.detected_count += 1\n",
    "                \n",
    "                self.avg_box [0][0] = min (self.avg_box [0][0], b [0][0])\n",
    "                self.avg_box [0][1] = min (self.avg_box [0][1], b [0][1])\n",
    "                self.avg_box [1][0] = max (self.avg_box [1][0], b [1][0])\n",
    "                self.avg_box [1][1] = max (self.avg_box [1][1], b [1][1])\n",
    "                \n",
    "                joined = True\n",
    "\n",
    "        return joined\n",
    "\n",
    "def calc_average_boxes (hot_boxes, strength):\n",
    "\n",
    "    avg_boxes = []\n",
    "    while len(hot_boxes) > 0:\n",
    "        b = hot_boxes.pop (0)\n",
    "        hb = HotBox (b)\n",
    "        while hb.join (hot_boxes):\n",
    "            pass\n",
    "        avg_boxes.append (hb)\n",
    "    \n",
    "    boxes = []\n",
    "    for ab in avg_boxes:\n",
    "        if ab.get_strength () >= strength:\n",
    "            boxes.append (ab.get_box ())\n",
    "    return boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Anaconda3\\envs\\py35\\lib\\site-packages\\skimage\\feature\\_hog.py:119: skimage_deprecation: Default value of `block_norm`==`L1` is deprecated and will be changed to `L2-Hys` in v0.15\n",
      "  'be changed to `L2-Hys` in v0.15', skimage_deprecation)\n"
     ]
    }
   ],
   "source": [
    "test_images = []\n",
    "test_images_titles = []\n",
    "\n",
    "for impath in glob.glob('test_images/test*.jpg'):\n",
    "    image_orig = mpimg.imread(impath)\n",
    "    \n",
    "\n",
    "    image = image_orig.astype(np.float32)/255\n",
    "\n",
    "    # hot boxes\n",
    "    hot_boxes, image_with_hot_boxes = get_hot_boxes (image)\n",
    "    # heat map\n",
    "    heat_map = get_heat_map (image, hot_boxes)\n",
    "    \n",
    "    # average boxes\n",
    "    avg_boxes = calc_average_boxes (hot_boxes, 2)\n",
    "    image_with_boxes = draw_boxes(image, avg_boxes, color=(0, 0, 1), thick=4)\n",
    "    \n",
    "    test_images.append (image_with_hot_boxes)\n",
    "    test_images.append (heat_map)\n",
    "    test_images.append (image_with_boxes)\n",
    "    \n",
    "    test_images_titles.extend (['', '', ''])\n",
    "    \n",
    "test_images_titles [0] = 'hot boxes'\n",
    "test_images_titles [1] = 'heat map'\n",
    "test_images_titles [2] = 'average boxes'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class LastHotBoxes:\n",
    "\n",
    "    def __init__ (self,max_len):\n",
    "        self.queue_max_len = max_len\n",
    "        self.last_boxes = []\n",
    "\n",
    "    def put_hot_boxes (self, boxes):\n",
    "\n",
    "        if (len(self.last_boxes) > self.queue_max_len):\n",
    "            tmp = self.last_boxes.pop (0)\n",
    "        \n",
    "        self.last_boxes.append (boxes)\n",
    "        \n",
    "    def get_hot_boxes (self):\n",
    "\n",
    "        b = []\n",
    "        for boxes in self.last_boxes:\n",
    "            b.extend (boxes)\n",
    "        return b\n",
    "\n",
    "last_hot_boxes = LastHotBoxes (15)\n",
    "    \n",
    "def process_image (image_orig):\n",
    "    \n",
    "    image_orig = np.copy (image_orig)\n",
    "    image = image_orig.astype(np.float32)/255\n",
    "\n",
    "    hot_boxes, image_with_hot_boxes = get_hot_boxes (image)\n",
    "    last_hot_boxes.put_hot_boxes (hot_boxes)\n",
    "    hot_boxes = last_hot_boxes.get_hot_boxes ()\n",
    "    \n",
    "\n",
    "    avg_boxes = calc_average_boxes (hot_boxes, 20)\n",
    "    image_with_boxes = draw_boxes(image, avg_boxes, color=(0, 0, 1), thick=4)\n",
    "\n",
    "    return image_with_boxes * 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MoviePy] >>>> Building video output/project_video_result.mp4\n",
      "[MoviePy] Writing audio in project_video_resultTEMP_MPY_wvf_snd.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████| 1112/1112 [00:01<00:00, 979.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MoviePy] Done.\n",
      "[MoviePy] Writing video output/project_video_result.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████▉| 1260/1261 [08:11<00:00,  2.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MoviePy] Done.\n"
     ]
    },
    {
     "ename": "PermissionError",
     "evalue": "[WinError 32] The process cannot access the file because it is being used by another process: 'project_video_resultTEMP_MPY_wvf_snd.mp3'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m<timed eval>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m<decorator-gen-174>\u001b[0m in \u001b[0;36mwrite_videofile\u001b[1;34m(self, filename, fps, codec, bitrate, audio, audio_fps, preset, audio_nbytes, audio_codec, audio_bitrate, audio_bufsize, temp_audiofile, rewrite_audio, remove_temp, write_logfile, verbose, threads, ffmpeg_params, progress_bar)\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\GATE\\AppData\\Roaming\\Python\\Python35\\site-packages\\moviepy\\decorators.py\u001b[0m in \u001b[0;36mrequires_duration\u001b[1;34m(f, clip, *a, **k)\u001b[0m\n\u001b[0;32m     52\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Attribute 'duration' not set\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclip\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     55\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<decorator-gen-173>\u001b[0m in \u001b[0;36mwrite_videofile\u001b[1;34m(self, filename, fps, codec, bitrate, audio, audio_fps, preset, audio_nbytes, audio_codec, audio_bitrate, audio_bufsize, temp_audiofile, rewrite_audio, remove_temp, write_logfile, verbose, threads, ffmpeg_params, progress_bar)\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\GATE\\AppData\\Roaming\\Python\\Python35\\site-packages\\moviepy\\decorators.py\u001b[0m in \u001b[0;36muse_clip_fps_by_default\u001b[1;34m(f, clip, *a, **k)\u001b[0m\n\u001b[0;32m    135\u001b[0m              for (k,v) in k.items()}\n\u001b[0;32m    136\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 137\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclip\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mnew_a\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mnew_kw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<decorator-gen-172>\u001b[0m in \u001b[0;36mwrite_videofile\u001b[1;34m(self, filename, fps, codec, bitrate, audio, audio_fps, preset, audio_nbytes, audio_codec, audio_bitrate, audio_bufsize, temp_audiofile, rewrite_audio, remove_temp, write_logfile, verbose, threads, ffmpeg_params, progress_bar)\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\GATE\\AppData\\Roaming\\Python\\Python35\\site-packages\\moviepy\\decorators.py\u001b[0m in \u001b[0;36mconvert_masks_to_RGB\u001b[1;34m(f, clip, *a, **k)\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mclip\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mismask\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[0mclip\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclip\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_RGB\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclip\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m@\u001b[0m\u001b[0mdecorator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecorator\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\GATE\\AppData\\Roaming\\Python\\Python35\\site-packages\\moviepy\\video\\VideoClip.py\u001b[0m in \u001b[0;36mwrite_videofile\u001b[1;34m(self, filename, fps, codec, bitrate, audio, audio_fps, preset, audio_nbytes, audio_codec, audio_bitrate, audio_bufsize, temp_audiofile, rewrite_audio, remove_temp, write_logfile, verbose, threads, ffmpeg_params, progress_bar)\u001b[0m\n\u001b[0;32m    350\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    351\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mremove_temp\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mmake_audio\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 352\u001b[1;33m             \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maudiofile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    353\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    354\u001b[0m         \u001b[0mverbose_print\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"[MoviePy] >>>> Video ready: %s \\n\\n\"\u001b[0m\u001b[1;33m%\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mPermissionError\u001b[0m: [WinError 32] The process cannot access the file because it is being used by another process: 'project_video_resultTEMP_MPY_wvf_snd.mp3'"
     ]
    }
   ],
   "source": [
    "from moviepy.editor import VideoFileClip\n",
    "\n",
    "def process_video (input_path, output_path):\n",
    "    clip = VideoFileClip (input_path)\n",
    "    \n",
    "\n",
    "\n",
    "    result = clip.fl_image (process_image)\n",
    "    %time result.write_videofile (output_path)\n",
    "\n",
    "# select video to operate on\n",
    "# process_video ('test_video.mp4', 'output/test_video_result.mp4')\n",
    "process_video ('project_video.mp4', 'output/project_video_result.mp4')\n",
    "# process_video ('challenge_video.mp4', 'output/challenge_video_result.mp4')\n",
    "# process_video ('harder_challenge_video.mp4', 'output/harder_challenge_video_result.mp4')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py35]",
   "language": "python",
   "name": "conda-env-py35-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
